{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLhrpnucMwiv"
      },
      "source": [
        "import gensim.downloader as api\n",
        "from sklearn.manifold import TSNE\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
        "\n",
        "import altair as alt\n",
        "\n",
        "!pip install datasets\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdvkbXYRbDXS"
      },
      "source": [
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZEFxW6nawYR"
      },
      "source": [
        "### Visualize the most frequent words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay7jZLQpNFuI"
      },
      "source": [
        "# take top 3000 words \n",
        "N = 5000\n",
        "subset_words = wv.index2entity[:N]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_U1NsrOSW70"
      },
      "source": [
        "vecs = np.zeros((N, 300))\n",
        "for i, w in enumerate(subset_words):\n",
        "  vecs[i,:] = wv[w]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o4lA253QLAy"
      },
      "source": [
        "tsne = TSNE(n_components=2, random_state=0, init='pca', learning_rate = 100)\n",
        "Y = tsne.fit_transform(vecs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKPWLUpwQSGp"
      },
      "source": [
        "df = pd.DataFrame(Y)\n",
        "df.columns = ['x', 'y']\n",
        "df[\"word\"] = subset_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqZ1pRnBSHXp"
      },
      "source": [
        "chart = alt.Chart(df).mark_circle().encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    tooltip=['word']).interactive().properties(\n",
        "    width=800,\n",
        "    height=800\n",
        ")\n",
        "\n",
        "chart.save('word2vec.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0FQwwDfbZ18"
      },
      "source": [
        "## Text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77dpXmmHTzSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6a048c-b1da-45c8-d54f-49efee79d386"
      },
      "source": [
        "# same as in tdidf notebook\n",
        "dataset = datasets.load_dataset(\"tweet_eval\", \"irony\")\n",
        "df_train = dataset[\"train\"].to_pandas()\n",
        "df_val = dataset[\"validation\"].to_pandas()\n",
        "df_test = dataset[\"test\"].to_pandas()\n",
        "\n",
        "def transform_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
        "  text = re.sub('\\s+(a|is|be|will|the|was|were|have|has|are|been|s|ll)\\s+', '', text)\n",
        "  return text\n",
        "\n",
        "def create_documents_list(l):\n",
        "  temp_vocab = [i.split(' ') for i in l]\n",
        "  documents_list = [[j for j in i if len(j)>0] for i in temp_vocab]\n",
        "  return documents_list\n",
        "\n",
        "df_train.text = df_train.text.apply(lambda x: transform_text(x))\n",
        "df_test.text = df_test.text.apply(lambda x: transform_text(x))\n",
        "df_val.text = df_val.text.apply(lambda x: transform_text(x))\n",
        "\n",
        "documents_list_train = create_documents_list(df_train.text.to_list())\n",
        "documents_list_val = create_documents_list(df_val.text.to_list())\n",
        "documents_list_test = create_documents_list(df_test.text.to_list())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/irony/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8He05cZUbqyK"
      },
      "source": [
        "# create function that takes average embeding per document\n",
        "def compute_embeddings_per_document(document, wv):\n",
        "  n_dims = wv[wv.index2entity[0]].shape[0]\n",
        "  result = np.zeros((len(document), n_dims))\n",
        "  for i, w in enumerate(document):\n",
        "    if w in wv:\n",
        "      result[i, :] = wv[w]\n",
        "  result = result.mean(axis=0)\n",
        "  return result \n",
        "\n",
        "def get_w2v_features(documents_list, wv):\n",
        "  n_dims = wv[wv.index2entity[0]].shape[0]\n",
        "  result = np.zeros((len(documents_list), n_dims))\n",
        "  for i, d in enumerate(documents_list):\n",
        "    result[i, :] = compute_embeddings_per_document(d, wv)\n",
        "  return result"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l82fNXA9b-Dq"
      },
      "source": [
        "X = get_w2v_features(documents_list_train, wv)\n",
        "y = df_train.label"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUqqPpANcgIq",
        "outputId": "127365ce-1811-4ef1-ca3a-f33b707fd490"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X, y )\n",
        "y_hat = clf.predict(X)\n",
        "f1_score(y,y_hat)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.665031534688157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvwldpdnc-jQ",
        "outputId": "15d0ff10-5bee-432c-e096-c37362fcf5ad"
      },
      "source": [
        "#validation\n",
        "y_hat_val = clf.predict(get_w2v_features(documents_list_val, wv))\n",
        "f1_score(df_val.label,y_hat_val)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6226622662266227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDtU09s3e3TH"
      },
      "source": [
        "# test\n",
        "y_hat_test = clf.predict(get_w2v_features(documents_list_test, wv))\n",
        "f1_score(df_test.label,y_hat_test)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaCN_62Ee7tf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}